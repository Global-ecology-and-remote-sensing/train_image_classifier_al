# Train Image Classifier Using Active Learning

The package provides scripts for training an image classifier
using active learning (AL). It attempts to improve the efficiency
of AL algorithms in an image processing context by training an
embedding model that reduces images to a low dimensional space
before active learning is performed. This approach has already
proven useful in a study by Norouzaddeh
et al. in training animal classifiers for camera trap
data (see [acknowledgements](#acknowledgements) for more 
details), however their methods are applicable to any image
classficiation problem. 

Norouzaddeh et al. provided the code that they used in their
experiment, however it lacked several features required to be
used in a practical setting. Most notably, it was neither able to
handle unlabelled data nor label that data. The package provided
here is an adaptation of their code which addresses these issues
and provides a pipeline for employing active learning with some
additional quality of life features.

The main pipeline provided by this package is a function called
"run_active_learning". It trains a model using the approach in
the paper by Norouzaddeh et al. with some additional features
such as providing checkpoints, testing the model and saving the 
model 
in an interoperable format.
If you wish to use your own custom model architecture and training
algorithm then this package could still prove useful by providing
objects that can load-in unlabelled data, label data and keeping
track of what images belong to which datasets (more information
can be found in 
[Designing your own model](#designing-your-own-model)).


**Comment** Why use active learning, active learning's slow computation on
raw images, package uses Norouzaddeh's approach in their paper, it was
adapted from the code that they used in their paper so that it can be
used outside of an experiment. It provides a main script "run_active_learning"
for running active learning but it also provides the objects, active_learning_environment,
data_set and engine which users can use for building their own image processing model.
It also uses both timelapse and AL algorithms designed by Google.

## Why Use Active Learning?

Training neural networks to classify images typically require very
large databases of manually labelled images. In some contexts,
labelling these images can be far more expensive than the
computational resources that are required to train the model. One
way to reduce this cost is to label the images as the model is
being trained. The idea behind this is to choose the best images
to label at each iteration of the training algorithm as to provide
the model with the most information at each stage. Algorithms that
employ this technique are often classed under "active learning".

## Quick Start

**Comment** What you need to run the program, the code that you need to run, 
how to prepare the data, how to label
things with timelapse. Using logging package to see info messages

This package can be installed locally using PIP. To do so, download 
the GitHub repository and run the following code, where the folder
given by "path\to\package" should contain the setup.py file

```python
pip install "path\to\package"
```

This package provides a function for training a model using 
active learning. The only information that it requires to
run are the file paths to the training and unlabelled data,
which must lie in separate directories. An example of how to run
the main program can be seen below.

```python
from camera_trap_al.main_scripts import run_active_learning

run_active_learning(
    train_data = "path\to\train\data",
    unlabelled_data = "path\to\unlabelled\data",
)
```

The program uses
PyTorch's ImageFolder class to read the data. As such, it
requires that images be sorted into subfolders by class. The
training data must contain all of the classes that could appear
in the unlabelled dataset as there is currently no way to add
classes to the data as the model is being trained. All unlabelled
data must also appear in subfolders of its root directory although 
the names of these folders will be ignored and this data will not
be considered to have been labelled. It is important to note that
any image data that lies directly in the data directories and not
inside a subfolder will be ignored. An example on how to structure 
the data can be found on their 
[website](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html).

The program will display loading bars for parts of the training
algorithm that are known to take a long time, such as during
finetuning and when extracting the image embedding. However,
there are other parts of the program that might take some time
but where progress bars cannot be generated. A log of when
these processes start and stop can be generated by running the
following code before the main function is run.

```python
import logging

logging.getLogger().setLevel(logging.INFO)
```

### Labelling Images

**Comment** Unfinished. Need to ask Kevin if I can edit the 
document that he gave me so that it has his name on it and that
no camera trap images appear on it.

As per the premise of active learning, the program will ask for
labels as the model is being trained. To do this, it will 
generate a CSV file in the top level of the 
[Active Learning Files](#active-learning-files-folder) folder.
The CSV will have the file name "timelapse_selector.csv" and have
the following columns:

- **File** The file name of the image.
- **RelativePath** path to the image relative root dir for 
unlabelled data (i.e. path passed to "unlabelled_data" parameter)
but without the file name.
- **Selected** Boolean. Entry is true if it needs a label and 
will be False otherwise.

The CSV will contain the file paths for all images in the 
unlabelled folder even if they have already been labelled by the 
program. The "Selected" column shows which images the program has
highlighted as needing labels. All selected images will need to
be labelled before the program can continue training the model.
It should also be noted that the program will ignore any labels 
for images that weren't selected (i.e. where "Selected" was 
False). It is also advised that you keep a record of any and all
labels that you make in case they are improperly recorded by
the program.

To label an image, add a column to the CSV called "Species" that
contains the class name for that image. The class name must
exactly match one of those given by the sub folders of the
"train_data" directory. After you have labelled all of the
selected images in the CSV, save it in the "new_labels_bin" 
folder of the "Active Learning Files" directory and press enter
on the command line. The saved file can have any file name as the
program will simply read the first CSV file that it finds in
there. If the program finds no issues with reading in the CSV
then it will display the message "Labels were loaded 
successfully" and continue training the model.

#### Labelling with Timelapse

The timelapse selector CSV was designed to be easily read by an
image labelling software known as Timelapse, although any image
labelling tool that adds a column called "Species" to a CSV would
work. Timelapse is an open-source program that is commonly used
by ecologists and animal conservation groups for analysing 
[camera trap](https://en.wikipedia.org/wiki/Camera_trap) data.
This package was orginally designed to help ecologists build
models that can sort databases of animal images by 
automating large parts of the training algorithm, which is why
it is desinged to work with Timelapse and why image labels need
to be placed in a "Species" column.

Details on how to 
[install and set up Timelapse](Timelapse User_Guide.pdf) 
for this package can be found on this GitHub page. 

**Comment** Need to add section on how to label images with
Timelapse

### Using a GPU

There is a known error when trying to use a GPU with this 
package. When the packages PyTorch and Torchvision are installed
while this package is being installed through pip, it will not
install the extension that is required to use these packages with
a GPU. As such, the program will not be able to find your GPU
should you have one in your machine. To work around this, please
uninstall torch and torchvision through pip and re-install it
with the required CUDA extention, which can be found on 
[PyTorch's website](https://pytorch.org/get-started/locally/). 
Of course, you will also need to install CUDA before you can use 
the PyTorch with CUDA extension, which can be found on [NVIDIA's
website](https://developer.nvidia.com/cuda-toolkit). If your
computer does not have an NVIDIA GPU then it currently cannot
be used by this package.

**Comment** Need to uninstall torch and torchvision and then re-install it with cuda.
Also need to install cuda and NVIDIA (maybe provide link to tutorial on using cuda and
NVIDIA)

## Training Algorithm

**Comment** Parameters of main script (No elaboration as that comes later), choices for training algorithm,
I think that when I elaborate, I should use pandas' example of have the parameter and then talk
about it?

The model that is trained by this package is actually two models
that are executed in sequence when they are evaluated. Images are
first passed through the embedding model which performs features
extraction and those features are then passed to the classifier
which makes the final decision. The reason behind splitting the
model into two is to reduce the computational cost of the
active learning process. 

The embedding model is the more complicated model and takes its
architecture and initial weights from a pre-trained neural 
network. The classifier on the other hand is a much simpler 
network with only two hidden layers. The embedding model is
periodically finetuned during training whereas the
classifier is retrained from scratch every time a batch of images
have been labelled. From 
[Norouzaddeh et al's study](#acknowledgements), performance
tends to improve greatly whenever the embedding model is updated
however there are only incrementaly gains in performance when
the classifier is re-trained.

Almost all of the default values for the hyperparameters are the
same as those found in Norouzaddeh et al.'s code for their
experiment. The following code snippet shows all of the parts
of the training algorithm that can be modified without changing
the source code. The rest of this section describes in more
detail what each of the parameters change. If the program starts
from a checkpoint, parameters that change the 
data sets and the model's architecture will be ignored.

```python
def run_active_learning(
    train_data,
    unlabelled_data,
    validation_data = None,
    validate_model : bool = True,
    use_checkpoints : bool = True,
    num_workers : int = 0,
    output_dir = 'default',
    active_batch : int = 100, 
    active_learning_strategy : str = 'margin',
    use_pretrained : bool = True,
    embedding_arch : str = 'resnet18',
    embedding_loss_type : str =  'triplet',
    embedding_loss_margin : float = 1.0,
    embedding_loss_data_strategy : str = 'random',
    feat_dim : int = 256,
    normalize_embedding : bool = True,
    extract_embedding_batch_size : int = 256,
    embedding_finetuning_period = 2000,
    embedding_finetuning_lr : float = 0.0001,
    embedding_finetuning_weight_decay = 0,
    embedding_finetuning_num_epochs : int = 20,
    embedding_finetuning_loader_type : str = 'balanced',
    embedding_train_lr : float = 0.00001,
    embedding_train_weight_decay = 0.0005,
    embedding_train_num_epochs : int = 5,
    embedding_train_loader_type : str = 'single',
    balanced_loader_num_classes : int = 20,
    balanced_loader_num_samples : int = 10,
)
```

### Data

**Comment** Format of data folders, train set needs all labels, unlabelled needs to be
in subfolders, validation set of classes can be a subset of the training ones

Ignored when loaded by checkpoint : True

All image files must lie in subfolders of their data directory.
Any images that lie in the top level of their directory will be
ignored. For labelled data, the names of the subfolders in the
top-level of the directory will be taken as the class name for
all images that are in those folders. The names of the subfolders
for the unlabelled directory will be ignored and no
top-level subfolders can contain no images. 
Additionally, There is
currently no way to add data to the AL program after the model
has been initially trained. 

**Parameters**:

- **train_data** : Data that the model is intially trained on
 before active learning is performed.
 Expects an absolute file path to a directory of images
 Subfolders of the top
 level of this directory must contain all of the classes that
 could be found in the unlabelled dataset. If you think that you
 might find images that do not belong to any of your classes then
 it is advised to add an "Other" class that these images could be
 assigned to.
 
- **unlabelled_data** : Data that has yet to be labelled. 
 Expects an absolute file path to a directory of images
 Images
 that are labelled through the active learning process will not
 be moved from this folder even if they would technically be
 considered as labelled at that point. Therefore, if a checkpoint
 is deleted and external record of the new labels have been made
 then those labels be lost. Furthermore, if you wish to retrain
 the model from scratch using labels that you had made from a
 previous execution of the program then an external program would
 be required to move those labelled images from the unlabelled
 dataset to the train dataset.
 
- **validation_data** : Data used to test the model as it is
 being trained. Expects an absolute file path to a directory of 
 images. Classes of this dataset can be a subset of those in the
 train data but the names of the classes that are included must
 exactly match those in the train data. The model is tested every
 time new labels are added to the data and the test results can
 be found in the [test results folder](#validation-results) of 
 the Active Learning Files folder.

### Training Loop Parameters

Ignored when loaded by checkpoint : False

**Parameters**:
 
- **validate_model** : Boolean. If True and if a test set exists,
 test results will be generated during training. Otherwise, no
 tests will be performed.

- **use_checkpoints** : If True if a checkpoint exists, function 
 will load model and data mappings from that checkpoint. 
 Otherwise, it will train a model from scratch. Checkpoints are 
 always overwritten by this program and so any progress made by 
 old checkpoints will be lost if this is set to False.

- **num_workers** : Number of workers that will train the 
 embeddingmodel
 and extract features in parallel. **WARNING** num_workers must 
 be set to 0 if running on a Windows machine or else the program
 will freeze indefinitely. This is because Windows OS blocks 
 multi-processing requests from PyTorch.

- **output_dir** : Directory where all files that are generated
 by the program will be stored. This includes checkpoints, label
 requests and the trained model. If 'default' is passed to the
 parameters then the program will create a folder in the working 
 directory called "Active Learning Files" within which the files
 will be stored. 

### Active learning

**Comment** AL batch size, available sampling methods

Ignored when loaded by checkpoint : False

Parameters that change how active learning is performed.

- **active_batch** : Number of images that the program will ask
 to be labelled at each pass of the training loop.
 
- **active_learning_strategy** : Strategy for choosing which 
 images to label when active learning is performed. can
 choose from the following list: 'uniform', 'graph_density', 
 'entropy', 'confidence', 'kcenter', 'margin', 
 'informative_diverse', 'margin_cluster_mean', 'hierarchical'.
 Note that only the 'margin' strategy has been tested. 

### Embedding model

**Comment** Available architectures, triplet loss vs softmax, 
(I don't know what the triplet loss hyperparameters do)

Ignored when loaded by checkpoint : True

Parameters that define the architecture and loss function of the
embedding model.

- **use_pretrained** : Boolean. If True, the program will 
 implement transfer learning by initialising the embedding model
 with the pre-trained weights of the base model.
 
- **embedding_arch** : Architecture of the embedding model.
 Only resnet, inception, densenet, vgg and alexnet architectures
 can be used by this package and using other architectures will
 likely cause errors to occur. When choosing an architecture,
 please pass the name of the architecture with its version number
 as a string. The name should be exactly the same as its model
 builder's name in the Torchvision models library. For a full
 list of the models in this library, pleas see their 
 [website](https://pytorch.org/vision/stable/models.html#classification)
 
- **embedding_loss_type** : Loss function of embedding model. Can
 either be 'softmax', 'triplet' or 'siamese'. Only 'softmax' and
 'triplet' have been tested.
 
- **embedding_loss_margin** : Margin for triplet and siamese 
 loss. Ignored if softmax loss is used.
 
- **embedding_loss_data_strategy** : Data selection strategy for 
 triplet and siamese loss. Can either be 'hardest', 'random', 
 'semi_hard' or 'hard_pair'. Ignored if loss type is 'softmax'.
 Note that if loss type is 'siamese' and data selection strategy
 is not 'hard_pair' then triplet loss will be used instead.

- **feat_dim** : Number of features that the embedding model 
 should extract from the images.

### Train and finetune embedding

**Comment** Adam sampler, available hyperparameters, importance of num_epochs, balanced vs simple loader

Ignored when loaded by checkpoint : False

- **normalize_embedding** : Boolean. If True, embedding values
 will be normalised. This avoids bias caused by dominating 
 features when active learning is performed. It is highly 
 recommended that this parameter be True unless you are certain 
 that the features extracted by the embedding model will be of 
 the same order of magnitude.
 
- **extract_embedding_batch_size** : Batch Size when features 
 are extracted from images.
 
- **embedding_finetuning_period** : Number of images to add 
 via active learning before the embedding is finetuned. The
 counter that keeps track of the number of images that has
 been added is reset after every finetuning.
 
The parameters that are titled embedding_finetuning_* and
embedding_train_* relate to the hyperparameters of the
algorithm that trains the embedding model. The difference
between them is that the embedding_train_* parameters are used 
when the model is initially trained on the training set and the
embedding_finetuning_* parameters are used after data has been
added through active learning. The leading asterisk in the
following parameters should be replaced with either 
"embedding_train" or "embedding_finetuning" before the
parameters are used.
 
- **\*_lr** : Learning rate of Adam optimiser.

- **\*_weight_decay** : Weight decay for Adam optimiser.

- **\*_num_epochs** : Number of epochs of the data to
 train/finetune the model on.
 
- **\*_loader_type** : Data sampling method during 
 training/finetuning. Can either be 'single' or 'balanced'. If 
 'single', the loader will simply shuffles the data with a batch 
 size of 128 and then sequentially load the data into the model.
 If 'balanced', images will be sampled so that the number of 
 images from each class in a batch are the same, and if 
 necessary, it will re-use images from a class if that class does
 not have enough images.

- **balanced_loader_num_classes** : Number of classes to sample 
 from at each batch of the balanced loader. Capped at number of 
 classes in train_data.

- **balanced_loader_num_samples** : Number of images to sample 
 from each class per batch of the balanced loader.

Note, batch size of balanced loader is num_classes * num_samples. 

### Classifier

**Comment** No way to edit classifier as it's hardcoded into the source code :(

Unfortunately, there is currently no way to change the 
hyperparameters of the classifier model as they are hard-coded
into the program. If you wish to change its architecture and how
it is trained then you will have to edit the source code.

By default, the classifier is a Neural Network with two hidden
layers. The first hidden layer consists of 230 nodes and the
second has 100 neurons. The size of the input layer is the same
as the dimension of the embedding and the output layer has the
same number of nodes as there are image classes. The program
will continue to train the model until either 2000 epochs 
have been reached or the difference in consecutive values of
the loss function is below 10<sup>-6</sup>. It is optimised
using the Adam method with a learning rate of 0.0001. All
other values are left the same as the default values in 
Scikit-Learn's 
[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) 
object.

### Data transformations

**Comment** Mention what data transformations are used during training inluding
normalisation. Say that to apply the model, data will need to be cropped and
normalised using the mean and std in the export folder

The program will perform several image transformations to the
data during training to improve the robustness of the model.
Unfortunately, these transformations cannot be changed as they 
are hard-coded into the program. Furthermore, the transformations
are precisely as follows:

1- Resize image to a 256*256 pixel square.

2- Crop the image with a 224*224 pixel square at a random part of
 the image.

3- Convert the image to grayscale with a 10% probability.

- Apply all of the following transformations in a random order.
 - Flip the image horizontally with a probability of 50%.
 - Randomly change the brightness, contrast and other aspects of
 the colour of the image according to PyTorch's 
 [ColourJitter](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)
 object.
 - Randomly rotate the image up to 20° in either direction.
 
4- Convert the image to a tensor.

5- Standardise the image pixel values according to the mean and
 the standard deviation of the the un-transformed pixels from
 all datasets.
 
Should you apply the model on a new dataset after it has been
trained, it is advised that you resize the image as per step 1,
crop the image at the centre to the same size as step 2, convert
the image to a tensor like in step-4 and then standardise the
pixel values as in step-5 before passing the data through the
model. The pixel mean and standard deviation matrices can be
found with the saved model weights in the 
[export folder](#exporting-the-model).

## Active Learning Files Folder

**Comment** This is where all files generated by the package are
saved

The main program will create several files and directories while
it trains the model. These are required to checkpoint its 
progress, load-in new labels and save the model in a format that
can be exported to another machine. Descriptions of these 
folders, as well as those for some other features, can be found
in this section.

### Checkpoint folders

**Comment** Needed for checkpoints, don't change or move these
folders if you want to load from checkpoint

The 'classifier', 'data' and 'embedding' folders contain all of
the files that are required to load the training process from a
checkpoint. Please do not remove any files from these folders or
from the ['export'](#exporting-the-model) folder or else the 
program will restart the training process from scratch.

The classifier folder contains the saved classifier model and a
record of which images lie in which dataset (i.e. train, 
unlabelled or validation). The classifier model file in this 
folder is saved using Python's pickle library and so cannot be
guaranteed to work on different machines. 

The data folder 
contains the pickled PyTorch dataset object which is used to
handle the data and the labels. It also contains a record of the
what images need labels when chosen through active learning and a
CSV file that lists labels for all images. If an image has yet to
be labelled, it's class will be listed as "unlabelled". Thus, to
avoid confusion it is recommended that no classes in the training
data should be called "unlabelled".

The embedding folder contains two files. The first is a matrix
which is the embedding values for all images across all datasets.
The second contains the model weights and other parameters that
define the embedding model. The file for the embedding model is
actually in a machine interoperable format but for convenience is
saved in both this folder and in the export folder.

### Label bin

**Comment** Covered by previous section but described here for completion

An initially empty folder whose sole purpose is to read in 
labels (see the section on [labelling images](#labelling-images)
for more details). The program does not remove any files in this
folder and so when labelling images it is recommended that you
overwrite the same CSV file every time. Furthermore, the program
will only look at files in the top level of the directory and
so CSV files contained in subfolders will be safely ignored.

### Validation results

**Comment** Stored in test results folder, to save model for
latest test results, you'll have to copy the export
folder before you submit the labels for that AL batch,
Does not exist if validation doesn't exist

If a validation dataset is provided, the program will test the
model every time before it asks for labels. The results of those
tests will be placed in subfolders of this folder. The title of
the folder details the total number of labelled images at the
time of the test and contained inside that folder is the 
confusion matrix and several test metrics.

The rows of the confusion matrix correspond to the images' true
labels and the columns are the labels that were predicted by the
model. The classes are ordered in alphabetical order from left
to right for columns and top to bottom for rows. If the
validation dataset contains only a subset of the classes in the
train set then it is possible that the confusion matrix will not
have rows and columns for all of the classes as it will omit
classes that have neither a true or predicted label. To see what
classes appear in the matrix, see the 'conf_mat_classes' section
of the metrics file. For more information on how the confusion
matrix is generated and formatted, see the documentation on the
[Scikit-Learn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) 
that the program uses.

The metrics file contains not only the classes of the confusion
matrix but also several performance metrics. These metrics are
precisely: accuracy, micro precision, micro recall, macro
precision and macro recall. Accuracy, precision and recall are
calculated with their usual definitions. The need to distinguish
between "micro" and "macro" precision and recall arises from an
ambiguity in their definitions in multi-class classification
tasks. Descriptions on how these metrics are calculated
can be found in the documentation of Scikit-Learn learn's
[precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)
and
[recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)
functions but in short, macro metrics are a simple average of
the score across the classes and micro metrics are the 
overall accuracy.

### Exporting the model

**Comment** Trained model for latest test results

This folder contains all of the files that you need to apply the
trained model on a new dataset. The folder contains four files.
The files dataset_mean.npy and dataset_std.npy should be loaded
with NumPy's [load function](https://numpy.org/doc/stable/reference/generated/numpy.load.html)
and are used to standardise the data before it gets fed into the
model, see the section on [transforming the data](#data-transformations) 
for more information. The file classifier.onnx is the classifier
model saved in ONNX format, instructions for using it can be 
found in [ONNX package's documentation](https://onnxruntime.ai/docs/get-started/with-python.html#scikit-learn-cv).
The last file is called "embedding_model_weights.pt" and can be
loaded using [PyTorch's load function](https://pytorch.org/tutorials/beginner/saving_loading_models.html#load).
It is a dictionary that contains all of the information required 
to recover the embedding model including: the trained weights,
dimension of the embedding and the model that it uses as a base.

It is recommended that you use the package 
["Animal Classifier Pipeline"](https://github.com/Global-ecology-and-remote-sensing/animal_classifier_pipeline)
to apply the model on a new dataset as it already has the
functions for doing so. However, that package relies on the
output of an animal detection model known as 
[Megadetector](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md)
and therefore cannot be directly used on non-animal data.
If your model is trained outside of an animal classification
context then you will have to design your own program for
applying the model. In that case, it may prove usefuly to look
at the source code of the 
[animal classifier pipeline](https://github.com/Global-ecology-and-remote-sensing/animal_classifier_pipeline)
to see how the model can be applied.

## Long processing times

**Comment** Building data loader and loading model from
checkpoint typically takes a long time so please be patient.

There are several processes in the training loop that are known
to take a long time. Processes such as training the embedding
model and extracting the embedding typically take a long time
and so the program will display progress bars while they run.
However, there are some processes that are known to take a long
time to run but where progress bars cannot be generated. 
Most notably, preparing the dataset before training typically
takes a long time as the program has to calculate the mean and
standard deviation of all the images. Additionally, loading the
program from a checkpoint can take a long time as the program
has to recover the state of the dataset using Python's pickle
library. These processes are performed before the main active
learning loop and so usually do not have a large impact on the
overall training time.

One part of the process that may become a problem is when the
classifier is trained. So far, the program that trains the
classifier has only been tested on training datasets of up to
5000 images. It may be possible that training the model for
larger datasets may be prohibitively expensive. In that case,
the only way to reduce the training time would be to change the
model's architecture or training algorithm in the source code.

## Designing your own model

**Comment** Brief comment on which classes might be useful
and that you can use run_active_learning.py as an example

There are several Python packages that let you train a model with
active learning. The 
[pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)
provides scripts for training PyTorch models with AL and the 
[modAL](https://github.com/modAL-python/modAL) package provides
a modular framework for training Scikit-Learn models. If you wish
to train a model on a low-dimensional dataset then these two
packages would likely prove to be more useful.

The package provided here attempts to improve the efficiency of
the AL training algorithm in an image processing context. It does
so by reducing the dimensionality of the data that the AL 
algorithms query from and by only training the final 
classification layers of the model after each batch of labels.
The primary purpose of the "run_active_learning" function is to
provide a way for people who are less familiar with machine
learning models a way to easily train a model while only needing
to manually label a small portion of their dataset.

Some of the functions and classes in this package can still prove
useful for those who would like to design their own PyTorch image
classifier using AL. In particular, the classes
"ExtendedImageFolder", "ActiveLearningEnvironment" and
"LabelRetriever" can prove useful for relabelling datasets,
keeping track of which images are labelled and interpreting new
labels respectively. Additionally, the source code of the
"run_active_learning" function provides an example of how to
train a model with active learning using checkpoints.

The following code can be used to import these classes after the
package has been installed.

```python
from camera_trap_al.deep_learning.data_loader import ExtendedImageFolder
from camera_trap_al.deep_learning.active_learning_manager import ActiveLearningEnvironment
from camera_trap_al.utils.objects import LabelRetriever
```

## Citation

**Comment** How to cite the package

This package is licensed under the Apache Licence 2.0. As such,
please cite it if you use it in your own project. when doing so,
please include the name of the author, Gareth Lamb, and a link to
the GitHub page.

## Contact

**Comment** Who to contact about the package and for HK trained models

## Acknowledgements

We would like to thank the team behind the animal detection 
algorithm, [MegaDetector](https://github.com/microsoft/CameraTraps),
for providing many of the functions for training and building 
the architecture for the embedding model that are used in this
package. The [GitHub page](https://github.com/microsoft/CameraTraps/tree/main/research/active_learning)
for their active learning program also includes the classes that
the "ExtendedImageFolder" and "ActiveLearningEnvironment" classes
in this package are adapted from. We would also like to thank
them for providing the basis for the active learning pipeline
that the main function of this package employs. Details of this
can be found in [their paper](https://doi.org/10.1111/2041-210X.13504), 
the details of which are as follows.

Image Processing Active Learning Paper
- **Title** - A deep active learning system for species identification and counting in camera trap images
- **Author** - Mohammad Sadegh Norouzzadeh, Dan Morris, Sara Beery1, Neel Joshi, Nebojsa Jojic, Jeff Clune
- **Journal** - Methods in Ecology and Evolution
- **Publisher** - British Ecological Society
- **Year** - 2020



**Comment** Acknwoledgments to Megadetector, Timelapse, Norouzaddeh,
Google team for AL algorithms